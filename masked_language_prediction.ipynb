{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked language prediction\n",
    "\n",
    "#### Starting from a redacted pdf, can we guess the redactions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start pdf-converter OCR service\n",
    "\n",
    "Using this tool:\n",
    "https://github.com/D2P-APPS/pdf-ocr-tool\n",
    "\n",
    "Run the following command to run the container and start the webservice:\n",
    "\n",
    "    docker-compose up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up BERT masked language prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting neighbors to a word in sentence using BERTMaskedLM. \n",
    "# Neighbors are from BERT vocab (which includes subwords and full words) \n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "\n",
    "DEFAULT_MODEL_PATH='bert-base-cased'\n",
    "DEFAULT_TO_LOWER=False\n",
    "DEFAULT_TOP_K = 10\n",
    "ACCRUE_THRESHOLD = 1\n",
    "\n",
    "def init_model(model_path,to_lower):\n",
    "    \"\"\"\n",
    "    Initiate BERTForMaskedLm model.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path,do_lower_case=to_lower)\n",
    "    model = BertForMaskedLM.from_pretrained(model_path)\n",
    "    #tokenizer = RobertaTokenizer.from_pretrained(model_path,do_lower_case=to_lower)\n",
    "    #model = RobertaForMaskedLM.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    return model,tokenizer\n",
    "\n",
    "\n",
    "def predict(model,tokenizer,top_k,accrue_threshold,text):\n",
    "    \"\"\"\n",
    "    Guess masked tokens.\n",
    "    \"\"\"\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Create the segments tensors.\n",
    "    segments_ids = [0] * len(tokenized_text)\n",
    "\n",
    "    masked_index = 0\n",
    "\n",
    "    for i in range(len(tokenized_text)):\n",
    "        if (tokenized_text[i] == \"[MASK]\"):\n",
    "            masked_index = i\n",
    "            break\n",
    "\n",
    "    #print(tokenized_text)\n",
    "    #print(masked_index)\n",
    "    results_dict = {}\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(tokens_tensor, segments_tensors)\n",
    "        for i in range(len(predictions[0][0,masked_index])):\n",
    "            if (float(predictions[0][0,masked_index][i].tolist()) > accrue_threshold):\n",
    "                tok = tokenizer.convert_ids_to_tokens([i])[0]\n",
    "                results_dict[tok] = float(predictions[0][0,masked_index][i].tolist())\n",
    "\n",
    "    k = 0\n",
    "    sorted_d = OrderedDict(sorted(results_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
    "    for i in sorted_d:\n",
    "        print(i,sorted_d[i])\n",
    "        k += 1\n",
    "        if (k > top_k):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model,tokenizer = init_model(DEFAULT_MODEL_PATH, to_lower=DEFAULT_TO_LOWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with a redacted PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"neg-bop-vol1-part2.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get text from PDF with OCR tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pdf\n",
    "\n",
    "# curl -X POST \"http://localhost:5001/convert\" -H \"accept: application/json\" -H \"Content-Type: multipart/form-data\" -F \"file=@neg-bop-vol1-part2.pdf;type=application/pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'docId': '66f618ed9b53579cd4b3cffaaced1d12-0832735',\n",
       " 'message': 'File successfully converted'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doc id\n",
    "\n",
    "{\n",
    "  \"docId\": \"66f618ed9b53579cd4b3cffaaced1d12-0832735\",\n",
    "  \"message\": \"File successfully converted\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text\n",
    "\n",
    "# curl -X GET \"http://localhost:5001/download?docId=66f618ed9b53579cd4b3cffaaced1d12-0832735&type=txt\" -H \"accept: application/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = 'bop.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(txt_file,\"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(orig_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample sentence from bay of pigs\n",
    "text = \"\"\"Although it cannot be determined accurately at\n",
    "what height any of the Brigade's B-26's actually were\n",
    "flying, Gar Thorsrud is of the opinion that they\n",
    "probably would have been cruising at 8,000'-10,000'\n",
    "for the early part of the trip, dropping down to\n",
    "2,000' when approximately 15 miles off the target\n",
    "by which time they would have been well past the\n",
    "Essex.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guess redactions with BERT masked language prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask \"height\"\n",
    "\n",
    "text = \"\"\"Although it cannot be determined accurately at\n",
    "what [MASK] any of the Brigade's B-26's actually were\n",
    "flying, Gar Thorsrud is of the opinion that they\n",
    "probably would have been cruising at 8,000'-10,000'\n",
    "for the early part of the trip, dropping down to\n",
    "2,000' when approximately 15 miles off the target\n",
    "by which time they would have been well past the\n",
    "Essex.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed 12.544036865234375\n",
      "time 11.864686965942383\n",
      "rate 11.43674087524414\n",
      "distance 10.931962966918945\n",
      "degree 10.52767562866211\n",
      "altitude 10.50318431854248\n",
      "range 10.494011878967285\n",
      "direction 10.279193878173828\n",
      "level 10.113349914550781\n",
      "position 9.554437637329102\n",
      "height 9.454315185546875\n"
     ]
    }
   ],
   "source": [
    "predict(model,tokenizer,DEFAULT_TOP_K,ACCRUE_THRESHOLD,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask B-52s\n",
    "\n",
    "text = \"\"\"Although it cannot be determined accurately at\n",
    "what height any of the [MASK] actually were\n",
    "flying, Gar Thorsrud is of the opinion that they\n",
    "probably would have been cruising at 8,000'-10,000'\n",
    "for the early part of the trip, dropping down to\n",
    "2,000' when approximately 15 miles off the target\n",
    "by which time they would have been well past the\n",
    "Essex.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "men 10.376850128173828\n",
      "bombers 9.472768783569336\n",
      "boats 9.46126651763916\n",
      "aircraft 9.459545135498047\n",
      "ships 9.444518089294434\n",
      "pilots 9.287113189697266\n",
      "planes 9.121543884277344\n",
      "three 8.551804542541504\n",
      "two 8.54004192352295\n",
      "tanks 8.48944091796875\n",
      "four 8.458709716796875\n"
     ]
    }
   ],
   "source": [
    "predict(model,tokenizer,DEFAULT_TOP_K,ACCRUE_THRESHOLD,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask \"target\"\n",
    "\n",
    "text = \"\"\"Although it cannot be determined accurately at\n",
    "what height any of the Brigade's B-26's actually were\n",
    "flying, Gar Thorsrud is of the opinion that they\n",
    "probably would have been cruising at 8,000'-10,000'\n",
    "for the early part of the trip, dropping down to\n",
    "2,000' when approximately 15 miles off the [MASK]\n",
    "by which time they would have been well past the\n",
    "Essex.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground 10.45962905883789\n",
      "water 10.02135181427002\n",
      "course 9.594242095947266\n",
      "coast 9.505855560302734\n",
      "river 8.977034568786621\n",
      "track 8.895634651184082\n",
      "air 8.824122428894043\n",
      "target 8.814438819885254\n",
      "wind 8.787298202514648\n",
      "road 8.736257553100586\n",
      "sea 8.680889129638672\n"
     ]
    }
   ],
   "source": [
    "predict(model,tokenizer,DEFAULT_TOP_K,ACCRUE_THRESHOLD,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how to identify redactions in pdf>txt conversion?\n",
    "#  - identify redaction indices\n",
    "#  - apply [MASK]s post-tokenization\n",
    "\n",
    "# TODO: tokens vs phrases \n",
    "#  - https://stackoverflow.com/questions/61419089/use-bert-to-predict-multiple-tokens\n",
    "#  - how much (% of text) do we need to redact before it's difficult to predict?\n",
    "\n",
    "# TODO: use trained model / add domain language to vocab\n",
    "# TODO: classification/boilerplate/junk text models\n",
    "# TODO: can we exploit the size of redaction?  number of characters?\n",
    "# TODO: bert vs roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
